{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMDBUJxeCC4I"
      },
      "outputs": [],
      "source": [
        "###########################   MULTI TRAINING ON CLEAN LOG   #########################################\n",
        "\n",
        "########################   TRAINING ON CLEAN LOG   ##################################\n",
        "\n",
        "\n",
        "\n",
        "import math, random, time, re, json\n",
        "from collections import defaultdict\n",
        "from typing import List, Tuple, Dict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from statsmodels import robust\n",
        "\n",
        "import os, pickle, pathlib, re\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "######### Configuration Constants\n",
        "SEED = 42\n",
        "LATENT_DIM = 32\n",
        "DROPOUT = 0.15\n",
        "LR = 1e-5\n",
        "BATCH = 64\n",
        "EPOCHS_MAX = 200\n",
        "PATIENCE = 12\n",
        "PCA_VAR = 0.97\n",
        "PCA_MAX_FRAC = 0.5\n",
        "JITTER_STD = 0.02\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "########### ----------------- helpers ----------------- ####\n",
        "def quat_angle_deg(q1, q2):\n",
        "    v = float(np.clip(abs(np.dot(q1, q2)), 0.0, 1.0))\n",
        "    ang = 2.0 * math.degrees(math.acos(v))\n",
        "    return ang if np.isfinite(ang) else 0.0\n",
        "\n",
        "def safe_mom(x):\n",
        "    x = np.asarray(x, float)\n",
        "    n = x.size\n",
        "    if n == 0:\n",
        "        return (0.0,) * 6\n",
        "    mean = float(np.mean(x))\n",
        "    if n < 2:\n",
        "        return (mean, 0.0, float(x.min()), float(x.max()), 0.0, 0.0)\n",
        "    std = float(np.std(x, ddof = 1))\n",
        "    mn, mx = float(x.min()), float(x.max())\n",
        "    if std <= 1e-12:\n",
        "        return (mean, std, mn, mx, 0.0, 0.0)\n",
        "    xc = x - mean\n",
        "    m2 = np.mean(xc ** 2)\n",
        "    m3 = np.mean(xc ** 3)\n",
        "    m4 = np.mean(xc ** 4)\n",
        "    skew = m3 / (m2 ** 1.5 + 1e-12)\n",
        "    kurt = m4 / (m2 ** 2 + 1e-12) - 3.0\n",
        "    if not np.isfinite(skew):\n",
        "        skew = 0.0\n",
        "    if not np.isfinite(kurt):\n",
        "        kurt = 0.0\n",
        "    return (mean, std, mn, mx, float(skew), float(kurt))\n",
        "\n",
        "def add_comp(prefix, arr, feat, cols):\n",
        "    if arr.size == 0:\n",
        "        arr = np.zeros((0, 3))\n",
        "    for j in range(arr.shape[1]):\n",
        "        for k, v in zip(\n",
        "            (\"mean\", \"std\", \"min\", \"max\", \"skew\", \"kurt\"),\n",
        "            safe_mom(arr[:, j]),\n",
        "        ):\n",
        "            feat.append(v)\n",
        "            cols.append(f\"{prefix}_{j}_{k}\")\n",
        "\n",
        "def add_scal(prefix, x, feat, cols):\n",
        "    for k, v in zip((\"mean\", \"std\", \"min\", \"max\", \"skew\", \"kurt\"), safe_mom(x)):\n",
        "        feat.append(v)\n",
        "        cols.append(f\"{prefix}_{k}\")\n",
        "\n",
        "####### ----------------- log parser ----------------- ########\n",
        "NUM_RE = re.compile(r\"[-+]?\\d+(?:\\.\\d+)?(?:[eE][-+]?\\d+)?\")\n",
        "\n",
        "def _nums(l):\n",
        "    return [float(t) for t in NUM_RE.findall(l)]\n",
        "\n",
        "def _its(x):\n",
        "    return int(round(x))\n",
        "\n",
        "def parse_log(path):\n",
        "    slows, fasts, imu = [], [], defaultdict(list)\n",
        "    with open(path, \"r\", errors = \"ignore\") as fh:\n",
        "        for ln in fh:\n",
        "            l = ln.strip()\n",
        "            if not l:\n",
        "                continue\n",
        "            up = l.upper()\n",
        "            nums = _nums(l)\n",
        "            if not nums:\n",
        "                continue\n",
        "            n = len(nums)\n",
        "            if (\"SLOW\" in up) or (n == 17):\n",
        "                tag = \"SLOW\"\n",
        "            elif (\"FAST\" in up) or (n == 23):\n",
        "                tag = \"FAST\"\n",
        "            elif (\"IMU\" in up) or (n == 7):\n",
        "                tag = \"IMU\"\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "            if tag == \"SLOW\":\n",
        "                slows.append(\n",
        "                    dict(\n",
        "                        ts = _its(nums[0]),\n",
        "                        ba = np.float32(nums[1:4]),\n",
        "                        bg = np.float32(nums[4:7]),\n",
        "                        pos = np.float32(nums[7:10]),\n",
        "                        vel = np.float32(nums[10:13]),\n",
        "                        quat = np.float32(nums[13:17]),\n",
        "                    )\n",
        "                )\n",
        "            elif tag == \"FAST\":\n",
        "                fasts.append(\n",
        "                    dict(\n",
        "                        ts = _its(nums[0]),\n",
        "                        pos = np.float32(nums[1:4]),\n",
        "                        vel = np.float32(nums[4:7]),\n",
        "                        quat = np.float32(nums[7:11]),\n",
        "                        b_acc = np.float32(nums[11:14]),\n",
        "                        b_gyro = np.float32(nums[14:17]),\n",
        "                        b_acc_prev = np.float32(nums[17:20]),\n",
        "                        b_gyro_prev = np.float32(nums[20:23]),\n",
        "                    )\n",
        "                )\n",
        "            else:\n",
        "                imu[_its(nums[0])].append(\n",
        "                    dict(w = np.float32(nums[1:4]), a = np.float32(nums[4:7]))\n",
        "                )\n",
        "\n",
        "    slows.sort(key = lambda d: d[\"ts\"])\n",
        "    fasts.sort(key = lambda d: d[\"ts\"])\n",
        "    return slows, fasts, imu\n",
        "\n",
        "#################### ----------------- feature builder -----------------##############\n",
        "def build(slows, fasts, imu) -> Tuple[pd.DataFrame, List[str]]:\n",
        "    fts = np.array([f[\"ts\"] for f in fasts], np.int64)\n",
        "    rows = []\n",
        "    cols_final = None  # <-- Initialize cols\n",
        "    for i, s in enumerate(slows[1:], 1):\n",
        "        prev, cur = slows[i - 1][\"ts\"], s[\"ts\"]\n",
        "        idx = np.nonzero((fts >= prev) & (fts < cur))[0]\n",
        "        if idx.size == 0:\n",
        "            continue\n",
        "        fw = [fasts[j] for j in idx]\n",
        "        feat, cols = [], []\n",
        "\n",
        "        dpos = np.stack([f[\"pos\"] - s[\"pos\"] for f in fw])\n",
        "        add_comp(\"dpos\", dpos, feat, cols)\n",
        "        add_scal(\"dpos_norm\", np.linalg.norm(dpos, 1), feat, cols)\n",
        "\n",
        "        dvel = np.stack([f[\"vel\"] - s[\"vel\"] for f in fw])\n",
        "        add_comp(\"dvel\", dvel, feat, cols)\n",
        "        add_scal(\"dvel_norm\", np.linalg.norm(dvel, 1), feat, cols)\n",
        "\n",
        "        add_scal(\"q_angle_deg\",\n",
        "                 [quat_angle_deg(f[\"quat\"], s[\"quat\"]) for f in fw],\n",
        "                 feat, cols)\n",
        "\n",
        "\n",
        "        for key in (\"b_acc\", \"b_gyro\", \"b_acc_prev\", \"b_gyro_prev\"):\n",
        "            for i, f in enumerate(fw):\n",
        "                arr = f[key]\n",
        "                if not isinstance(arr, np.ndarray) or arr.shape != (3,):\n",
        "                    print(f\"[DEBUG] At index {i}, key='{key}', bad shape: {arr.shape}, value: {arr}\")\n",
        "            add_comp(key, np.stack([f[key] for f in fw]), feat, cols)\n",
        "\n",
        "        iw, ia = [], []\n",
        "        for f in fw:\n",
        "            for sm in imu.get(int(f[\"ts\"]), []):\n",
        "                iw.append(sm[\"w\"])\n",
        "                ia.append(sm[\"a\"])\n",
        "        add_comp(\"imu_w\", np.stack(iw) if iw else np.zeros((0, 3)), feat, cols)\n",
        "        add_scal(\"imu_w_norm\",\n",
        "                 np.linalg.norm(iw, 1) if iw else np.zeros(0), feat, cols)\n",
        "        add_comp(\"imu_a\", np.stack(ia) if ia else np.zeros((0, 3)), feat, cols)\n",
        "        add_scal(\"imu_a_norm\",\n",
        "                 np.linalg.norm(ia, 1) if ia else np.zeros(0), feat, cols)\n",
        "\n",
        "        for k in (\"pos\", \"vel\", \"ba\", \"bg\"):\n",
        "            for j, v in enumerate(s[k]):\n",
        "                feat.append(float(v))\n",
        "                cols.append(f\"slow_{k}_{j}\")\n",
        "\n",
        "        feat.insert(0, (cur - prev) / 1e9)\n",
        "        cols.insert(0, \"win_dur_s\")\n",
        "        feat.insert(0, float(len(idx)))\n",
        "        cols.insert(0, \"win_n_fast\")\n",
        "        rows.append(feat)\n",
        "\n",
        "        if cols_final is None:\n",
        "            cols_final = cols  # Save first valid columns\n",
        "\n",
        "    if not rows or cols_final is None:\n",
        "        return pd.DataFrame(), []  # Nothing to build\n",
        "\n",
        "    df = pd.DataFrame(rows, columns = cols)\n",
        "    return df, cols\n",
        "\n",
        "# ----------------- AE -----------------\n",
        "'''\n",
        "class AE(nn.Module):\n",
        "    def __init__(self, d, lat = LATENT_DIM, drop = DROPOUT):\n",
        "        super().__init__()\n",
        "        self.enc = nn.Sequential(\n",
        "            nn.Linear(d, 256), nn.ReLU(),\n",
        "            nn.Dropout(drop),\n",
        "            nn.Linear(256, 128), nn.ReLU(),\n",
        "            nn.Linear(128, lat),\n",
        "        )\n",
        "        self.dec = nn.Sequential(\n",
        "            nn.Linear(lat, 128), nn.ReLU(),\n",
        "            nn.Linear(128, 256), nn.ReLU(),\n",
        "            nn.Dropout(drop),\n",
        "            nn.Linear(256, d),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.dec(self.enc(x))\n",
        "'''\n",
        "'''\n",
        "class AE(nn.Module):\n",
        "    def __init__(self, d, lat = LATENT_DIM, drop = DROPOUT):\n",
        "        super().__init__()\n",
        "        self.enc = nn.Sequential(\n",
        "            nn.Linear(d, 128), nn.ReLU(),\n",
        "            nn.Linear(128, 64), nn.ReLU(),\n",
        "            nn.Linear(64,  lat))\n",
        "        self.dec = nn.Sequential(\n",
        "            nn.Linear(lat, 64), nn.ReLU(),\n",
        "            nn.Linear(64, 128), nn.ReLU(),\n",
        "            nn.Linear(128, d))\n",
        "    def forward(self, x):\n",
        "        return self.dec(self.enc(x))\n",
        "'''\n",
        "\n",
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.kaiming_uniform_(m.weight, nonlinearity='leaky_relu')\n",
        "        if m.bias is not None:\n",
        "            nn.init.zeros_(m.bias)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "class AE(nn.Module):\n",
        "    def __init__(self, d, lat=LATENT_DIM):\n",
        "        super().__init__()\n",
        "        self.enc = nn.Sequential(\n",
        "            nn.Linear(d, 256), nn.BatchNorm1d(256), nn.LeakyReLU(0.01),\n",
        "            nn.Linear(256, 128), nn.BatchNorm1d(128), nn.LeakyReLU(0.01),\n",
        "            nn.Linear(128, 64), nn.BatchNorm1d(64), nn.LeakyReLU(0.01),\n",
        "            nn.Linear(64, lat)\n",
        "        )\n",
        "        self.dec = nn.Sequential(\n",
        "            nn.Linear(lat, 64), nn.LeakyReLU(0.01),\n",
        "            nn.Linear(64, 128), nn.LeakyReLU(0.01),\n",
        "            nn.Linear(128, 256), nn.LeakyReLU(0.01),\n",
        "            nn.Linear(256, d),\n",
        "            nn.Tanh()  ##### new\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = self.enc(x)\n",
        "        #print(\"Latent mean/std:\", z.mean().item(), z.std().item())  # ðŸ§ª Add this\n",
        "        return self.dec(z)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def train_multi_logs(log_paths: List[str]):\n",
        "    log_paths.sort()\n",
        "    train_logs = log_paths[:90]\n",
        "    val_logs = log_paths[90:]\n",
        "\n",
        "    # Save log split manifest\n",
        "    out_dir = pathlib.Path(\"ae_parameters_multi\")\n",
        "    out_dir.mkdir(exist_ok=True)\n",
        "    json.dump({\"train_logs\": train_logs, \"val_logs\": val_logs}, open(out_dir / \"split_manifest.json\", \"w\"), indent=2)\n",
        "\n",
        "    def extract_df(log_list):\n",
        "        dfs = []\n",
        "        for path in log_list:\n",
        "            slows, fasts, imu = parse_log(path)\n",
        "            df, cols = build(slows, fasts, imu)\n",
        "            if not df.empty:\n",
        "                dfs.append(df)\n",
        "        return pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "    df_train = extract_df(train_logs)\n",
        "    df_val = extract_df(val_logs)\n",
        "\n",
        "    ###### Drop unstable features (tested to see htat these dont generalize well)\n",
        "    cols_filtered = [c for c in df_train.columns if not any(k in c for k in (\n",
        "        \"skew\", \"kurt\", \"slow_pos\", \"slow_vel\", \"win_n_fast\", \"win_dur_s\"))]\n",
        "\n",
        "    def apply_jitter(X):\n",
        "        return X + np.random.normal(0, JITTER_STD, X.shape).astype(np.float32)\n",
        "\n",
        "    X_train = apply_jitter(df_train[cols_filtered].values.astype(np.float32)) #============================================\n",
        "    #X_train = df_train[cols_filtered].values.astype(np.float32)  # no jitter\n",
        "    X_val = df_val[cols_filtered].values.astype(np.float32)\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_val_scaled = scaler.transform(X_val)\n",
        "\n",
        "    X_train_scaled = np.clip(X_train_scaled, -6.0, 6.0)\n",
        "    X_val_scaled = np.clip(X_val_scaled, -6.0, 6.0)\n",
        "\n",
        "    #### PCA\n",
        "    pca_full = PCA().fit(X_train_scaled)\n",
        "    k = min(\n",
        "        np.searchsorted(np.cumsum(pca_full.explained_variance_ratio_), PCA_VAR) + 1,\n",
        "        X_train_scaled.shape[1], int(PCA_MAX_FRAC * len(X_train_scaled))\n",
        "    )\n",
        "\n",
        "    print(f\"\\n PCA Feature Reduction:\")\n",
        "    print(f\"Original feature count : {X_train_scaled.shape[1]}\")\n",
        "    print(f\"Reduced feature count  : {int(k)} (explaining {PCA_VAR*100:.1f}% variance)\")\n",
        "\n",
        "    plt.plot(np.cumsum(pca_full.explained_variance_ratio_))\n",
        "    plt.axhline(PCA_VAR, color=\"r\", linestyle=\"--\", label=f\"{PCA_VAR:.2%} target\")\n",
        "    plt.xlabel(\"Number of PCA components\")\n",
        "    plt.ylabel(\"Cumulative explained variance\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.title(\"Explained Variance vs PCA Components\")\n",
        "    plt.show()\n",
        "\n",
        "    pca = PCA(n_components=k, whiten=True)  # Whitening OFF for now\n",
        "    Z_train = pca.fit_transform(X_train_scaled)\n",
        "    Z_val = pca.transform(X_val_scaled)\n",
        "\n",
        "    tr_dl = DataLoader(TensorDataset(torch.tensor(Z_train)), batch_size=BATCH, shuffle=True)\n",
        "    va_dl = DataLoader(TensorDataset(torch.tensor(Z_val)), batch_size=BATCH)\n",
        "\n",
        "    ############ Building AE model\n",
        "    ae = AE(int(k))\n",
        "    ae.apply(init_weights)\n",
        "    opt = torch.optim.AdamW(ae.parameters(), lr=LR, weight_decay=1e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=EPOCHS_MAX)\n",
        "    crit = nn.SmoothL1Loss(beta=0.5)\n",
        "\n",
        "    ###### Diagnostic (checking values) before training\n",
        "    xb_sample = next(iter(tr_dl))[0]\n",
        "    ae.eval()\n",
        "    with torch.no_grad():\n",
        "        out_sample = ae(xb_sample)\n",
        "    print(\"Input mean/std :\", xb_sample.mean().item(), xb_sample.std().item())\n",
        "    print(\"Output mean/std:\", out_sample.mean().item(), out_sample.std().item())\n",
        "    print(\"First row MSE  :\", torch.mean((xb_sample[0] - out_sample[0]) ** 2).item())\n",
        "    print(\"Reconstruction delta:\", torch.abs(out_sample - xb_sample).mean().item())\n",
        "    plt.plot(xb_sample[0].cpu().numpy(), label='Input')\n",
        "    plt.plot(out_sample[0].cpu().numpy(), label='Reconstructed')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    best, bad = float(\"inf\"), 0\n",
        "    best_state = None\n",
        "    train_losses, val_losses = [], []\n",
        "\n",
        "    for ep in range(1, EPOCHS_MAX + 1):\n",
        "        ae.train()\n",
        "        tr_loss = sum(crit(ae(xb), xb).item() * xb.size(0) for xb, in tr_dl) / len(Z_train)\n",
        "        ae.eval()\n",
        "        va_loss = sum(crit(ae(xb), xb).item() * xb.size(0) for xb, in va_dl) / len(Z_val)\n",
        "        train_losses.append(tr_loss)\n",
        "        val_losses.append(va_loss)\n",
        "        print(f\"ep{ep:03d} tr={tr_loss:.4f} va={va_loss:.4f} bad={bad}/{PATIENCE}\")\n",
        "        if va_loss < best - 1e-5:\n",
        "            best, bad = va_loss, 0\n",
        "            best_state = ae.state_dict()\n",
        "        else:\n",
        "            bad += 1\n",
        "            if bad >= PATIENCE:\n",
        "                break\n",
        "        scheduler.step()  ###### update learning rate\n",
        "\n",
        "\n",
        "\n",
        "    ae.load_state_dict(best_state)\n",
        "    ae.eval()\n",
        "    Z_val_tensor = torch.tensor(Z_val)\n",
        "    with torch.no_grad():\n",
        "        mse_val = torch.mean((ae(Z_val_tensor) - Z_val_tensor) ** 2, dim=1).numpy()\n",
        "\n",
        "    ##### Thresholds\n",
        "    thr_med = float(np.median(mse_val))\n",
        "    thr_p98 = float(np.percentile(mse_val, 98))\n",
        "    thr_p99 = float(np.percentile(mse_val, 99))\n",
        "    thr_mad = float(thr_med + 3 * robust.mad(mse_val))\n",
        "\n",
        "    print(\"\\n[Thresholds from validation set]\")\n",
        "    print(\"Median:\", thr_med)\n",
        "    print(\"p98   :\", thr_p98)\n",
        "    print(\"p99   :\", thr_p99)\n",
        "    print(\"MAD   :\", thr_mad)\n",
        "\n",
        "    #### Save parameters\n",
        "    pickle.dump(scaler, open(out_dir / \"scaler.pkl\", \"wb\"))\n",
        "    pickle.dump(pca, open(out_dir / \"pca.pkl\", \"wb\"))\n",
        "    json.dump(cols_filtered, open(out_dir / \"cols.json\", \"w\"))\n",
        "    json.dump(dict(thr_mad=thr_mad, thr_p98=thr_p98, thr_p99=thr_p99, thr_med=thr_med),\n",
        "               open(out_dir / \"thresholds.json\", \"w\"))\n",
        "    torch.jit.script(ae.eval()).save(out_dir / \"ae.pt\")\n",
        "    print(\"\\nâœ… Saved model and thresholds to\", out_dir)\n",
        "\n",
        "    return {\n",
        "        \"mse\": mse_val,\n",
        "        \"thr_med\": thr_med,\n",
        "        \"thr_p98\": thr_p98,\n",
        "        \"thr_p99\": thr_p99,\n",
        "        \"thr_mad\": thr_mad,\n",
        "        \"df\": df_val.reset_index(drop=True)\n",
        "    }\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    log_paths = [f\"clean_logs/normal_pose_append_log_{i:03d}.txt\" for i in range(1, 101)]\n",
        "    art = train_multi_logs(log_paths)\n",
        "\n",
        "    # Attach MSE to dataframe\n",
        "    df_new = art[\"df\"]\n",
        "    mse = art[\"mse\"]\n",
        "    df_new[\"mse\"] = mse\n",
        "\n",
        "    # Plot histogram\n",
        "    plt.hist(mse, bins=100, color='skyblue', edgecolor='black')\n",
        "    plt.axvline(art[\"thr_med\"], color='green', linestyle='--', label='median')\n",
        "    plt.axvline(art[\"thr_p98\"], color='red', linestyle='--', label='p98')\n",
        "    plt.axvline(art[\"thr_p99\"], color='blue', linestyle='--', label='p99')\n",
        "    plt.axvline(art[\"thr_mad\"], color='yellow', linestyle='--', label='mad')\n",
        "    plt.legend()\n",
        "    plt.title(\"Reconstruction Error Histogram\")\n",
        "    plt.xlabel(\"MSE\")\n",
        "    plt.ylabel(\"Log Frequency\")\n",
        "    plt.yscale(\"log\")\n",
        "    plt.xlim(0, 2)\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    ##### Detect anomalies\n",
        "    anomalies_p98 = np.where(mse > art[\"thr_p98\"])[0]\n",
        "    anomalies_p99 = np.where(mse > art[\"thr_p99\"])[0]\n",
        "    anomalies_mad = np.where(mse > art[\"thr_mad\"])[0]\n",
        "\n",
        "    print(f\"\\n[Anomalies]\")\n",
        "    print(f\"p98 threshold: {len(anomalies_p98)} anomalies\")\n",
        "    print(f\"p99 threshold: {len(anomalies_p99)} anomalies\")\n",
        "    print(f\"MAD threshold: {len(anomalies_mad)} anomalies\")\n",
        "\n",
        "    ##### Hybrid labeling\n",
        "    df_new[\"hybrid_anomaly_level\"] = np.select(\n",
        "        [\n",
        "            df_new[\"mse\"] > art[\"thr_p98\"],\n",
        "            df_new[\"mse\"] > art[\"thr_mad\"]\n",
        "        ],\n",
        "        [\"hard\", \"soft\"],\n",
        "        default=\"normal\"\n",
        "    )\n",
        "\n",
        "    ##### Count and show\n",
        "    print(\"\\n[Hybrid Anomaly Classification]\")\n",
        "    hybrid_counts = df_new[\"hybrid_anomaly_level\"].value_counts()\n",
        "    for level in [\"normal\", \"soft\", \"hard\"]:\n",
        "        count = hybrid_counts.get(level, 0)\n",
        "        print(f\"  {level:>6}: {count} / {len(df_new)} ({100*count/len(df_new):.2f}%)\")\n",
        "\n",
        "    ##### Plot with seaborn\n",
        "    sns.histplot(data=df_new, x=\"mse\", hue=\"hybrid_anomaly_level\", bins=100, log_scale=True)\n",
        "    plt.axvline(art[\"thr_p98\"], color=\"red\", linestyle=\"--\", label=\"p98\")\n",
        "    plt.axvline(art[\"thr_mad\"], color=\"yellow\", linestyle=\"--\", label=\"mad\")\n",
        "    plt.legend()\n",
        "    plt.title(\"MSE Histogram by Hybrid Anomaly Level\")\n",
        "    plt.xlabel(\"MSE\")\n",
        "    plt.ylabel(\"Log Count\")\n",
        "    plt.show()\n",
        "\n",
        "    ##### Top anomalies\n",
        "    print(\"\\nTop 10 anomalies by MSE:\")\n",
        "    top_anomalies = df_new.sort_values(by=\"mse\", ascending=False).head(10)\n",
        "    print(top_anomalies[[\"mse\"] + df_new.columns[:5].tolist()])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "######## --- Load & preprocess log --- #########\n",
        "log_path = \"/home/jarvis/pose_append_log.csv\"  #### Replace as needed\n",
        "slows, fasts, imu = parse_log(log_path)\n",
        "df_eval, _ = build(slows, fasts, imu)\n",
        "\n",
        "import pickle\n",
        "import json\n",
        "\n",
        "##### Load scaler, PCA, and column list\n",
        "with open(\"ae_parameters_multi/scaler.pkl\", \"rb\") as f:\n",
        "    scaler = pickle.load(f)\n",
        "\n",
        "with open(\"ae_parameters_multi/pca.pkl\", \"rb\") as f:\n",
        "    pca = pickle.load(f)\n",
        "\n",
        "with open(\"ae_parameters_multi/cols.json\", \"r\") as f:\n",
        "    cols_filtered = json.load(f)\n",
        "\n",
        "ae = torch.jit.load(\"ae_parameters_multi/ae.pt\")\n",
        "ae.eval()\n",
        "\n",
        "\n",
        "###### Ensure valid samples\n",
        "if df_eval.empty:\n",
        "    print(\"âš ï¸ No usable windows in the evaluation log.\")\n",
        "else:\n",
        "    cols_filtered = [c for c in df_eval.columns if not any(k in c for k in (\n",
        "        \"skew\", \"kurt\", \"slow_pos\", \"slow_vel\", \"win_n_fast\", \"win_dur_s\"))]\n",
        "    #### Filter & scale\n",
        "    X_eval = df_eval[cols_filtered].values.astype(np.float32)\n",
        "    X_eval_scaled = scaler.transform(X_eval)\n",
        "    X_eval_scaled = np.clip(X_eval_scaled, -6.0, 6.0)\n",
        "\n",
        "    #### Apply PCA\n",
        "    Z_eval = pca.transform(X_eval_scaled)\n",
        "\n",
        "    #### Do Inference\n",
        "    Z_eval_tensor = torch.tensor(Z_eval, dtype=torch.float32)\n",
        "    ae.eval()\n",
        "    with torch.no_grad():\n",
        "        Z_recon = ae(Z_eval_tensor)\n",
        "        mse_eval = torch.mean((Z_eval_tensor - Z_recon) ** 2, dim=1).numpy()\n",
        "\n",
        "    ##### Classify anomalies using saved thresholds\n",
        "    with open(\"ae_parameters_multi/thresholds.json\", \"r\") as f:\n",
        "        thr = json.load(f)\n",
        "\n",
        "    df_eval[\"mse\"] = mse_eval\n",
        "    df_eval[\"hybrid_anomaly_level\"] = np.select(\n",
        "        [\n",
        "            df_eval[\"mse\"] > thr[\"thr_p98\"],\n",
        "            df_eval[\"mse\"] > thr[\"thr_mad\"]\n",
        "        ],\n",
        "        [\"hard\", \"soft\"],\n",
        "        default=\"normal\"\n",
        "    )\n",
        "\n",
        "    ##### Results summary ####\n",
        "    counts = df_eval[\"hybrid_anomaly_level\"].value_counts()\n",
        "    total = len(df_eval)\n",
        "    print(f\"\\nâœ… Inference done on {log_path}\")\n",
        "    for level in [\"normal\", \"soft\", \"hard\"]:\n",
        "        count = counts.get(level, 0)\n",
        "        print(f\"{level:>6}: {count:5d} / {total} ({100*count/total:.2f}%)\")\n",
        "\n",
        "    #### Plot MSE histogram\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "\n",
        "    sns.histplot(data=df_eval, x=\"mse\", hue=\"hybrid_anomaly_level\", bins=100, log_scale=True)\n",
        "    plt.axvline(thr[\"thr_p98\"], color=\"red\", linestyle=\"--\", label=\"p98\")\n",
        "    plt.axvline(thr[\"thr_mad\"], color=\"yellow\", linestyle=\"--\", label=\"mad\")\n",
        "    plt.legend()\n",
        "    plt.title(\"MSE Histogram (Eval Log)\")\n",
        "    plt.xlabel(\"MSE\")\n",
        "    plt.ylabel(\"Log Count\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    #### Preview top anomalies ####\n",
        "    top_anomalies = df_eval.sort_values(by=\"mse\", ascending=False).head(10)\n",
        "    display(top_anomalies[[\"mse\"] + df_eval.columns[:5].tolist()])\n"
      ],
      "metadata": {
        "id": "buQiJCNpDpxp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}